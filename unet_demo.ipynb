{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: torch.Size([8, 3, 224, 224])\n",
      "Out: torch.Size([8, 3, 224, 224])\n",
      "Features: torch.Size([8, 128, 56, 56])\n"
     ]
    }
   ],
   "source": [
    "from unet import UNet\n",
    "import torch\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
    "# Unet has skip connections removed\n",
    "model = UNet(in_channels=3,\n",
    "             out_channels=3,\n",
    "             n_blocks=3,\n",
    "             start_filters=32,\n",
    "             activation='relu',\n",
    "             normalization='batch',\n",
    "             conv_mode='same',\n",
    "             dim=2)\n",
    "model = model.to(device)\n",
    "\n",
    "input = torch.randn(size=(8, 3, 224, 224), dtype=torch.float32)\n",
    "input = input.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "  out = model(input, encode_only=False)\n",
    "  features = model(input, encode_only=True)\n",
    "\n",
    "print(f'Input: {input.shape}')\n",
    "print(f'Out: {out.shape}')\n",
    "print(f'Features: {features.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 224, 224]             896\n",
      "              ReLU-2         [-1, 32, 224, 224]               0\n",
      "       BatchNorm2d-3         [-1, 32, 224, 224]              64\n",
      "            Conv2d-4         [-1, 32, 224, 224]           9,248\n",
      "              ReLU-5         [-1, 32, 224, 224]               0\n",
      "       BatchNorm2d-6         [-1, 32, 224, 224]              64\n",
      "         MaxPool2d-7         [-1, 32, 112, 112]               0\n",
      "         DownBlock-8  [[-1, 32, 112, 112], [-1, 32, 224, 224]]               0\n",
      "            Conv2d-9         [-1, 64, 112, 112]          18,496\n",
      "             ReLU-10         [-1, 64, 112, 112]               0\n",
      "      BatchNorm2d-11         [-1, 64, 112, 112]             128\n",
      "           Conv2d-12         [-1, 64, 112, 112]          36,928\n",
      "             ReLU-13         [-1, 64, 112, 112]               0\n",
      "      BatchNorm2d-14         [-1, 64, 112, 112]             128\n",
      "        MaxPool2d-15           [-1, 64, 56, 56]               0\n",
      "        DownBlock-16  [[-1, 64, 56, 56], [-1, 64, 112, 112]]               0\n",
      "           Conv2d-17          [-1, 128, 56, 56]          73,856\n",
      "             ReLU-18          [-1, 128, 56, 56]               0\n",
      "      BatchNorm2d-19          [-1, 128, 56, 56]             256\n",
      "           Conv2d-20          [-1, 128, 56, 56]         147,584\n",
      "             ReLU-21          [-1, 128, 56, 56]               0\n",
      "      BatchNorm2d-22          [-1, 128, 56, 56]             256\n",
      "        DownBlock-23  [[-1, 128, 56, 56], [-1, 128, 56, 56]]               0\n",
      "  ConvTranspose2d-24         [-1, 64, 112, 112]          32,832\n",
      "             ReLU-25         [-1, 64, 112, 112]               0\n",
      "      BatchNorm2d-26         [-1, 64, 112, 112]             128\n",
      "           Conv2d-27         [-1, 64, 112, 112]          36,928\n",
      "             ReLU-28         [-1, 64, 112, 112]               0\n",
      "      BatchNorm2d-29         [-1, 64, 112, 112]             128\n",
      "           Conv2d-30         [-1, 64, 112, 112]          36,928\n",
      "             ReLU-31         [-1, 64, 112, 112]               0\n",
      "      BatchNorm2d-32         [-1, 64, 112, 112]             128\n",
      "          UpBlock-33         [-1, 64, 112, 112]               0\n",
      "  ConvTranspose2d-34         [-1, 32, 224, 224]           8,224\n",
      "             ReLU-35         [-1, 32, 224, 224]               0\n",
      "      BatchNorm2d-36         [-1, 32, 224, 224]              64\n",
      "           Conv2d-37         [-1, 32, 224, 224]           9,248\n",
      "             ReLU-38         [-1, 32, 224, 224]               0\n",
      "      BatchNorm2d-39         [-1, 32, 224, 224]              64\n",
      "           Conv2d-40         [-1, 32, 224, 224]           9,248\n",
      "             ReLU-41         [-1, 32, 224, 224]               0\n",
      "      BatchNorm2d-42         [-1, 32, 224, 224]              64\n",
      "          UpBlock-43         [-1, 32, 224, 224]               0\n",
      "           Conv2d-44          [-1, 3, 224, 224]              99\n",
      "================================================================\n",
      "Total params: 421,987\n",
      "Trainable params: 421,987\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 7375553.88\n",
      "Params size (MB): 1.61\n",
      "Estimated Total Size (MB): 7375556.07\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
    "model = model.to(device)\n",
    "\n",
    "summary = summary(model, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 1: 112.0\n",
      "Level 2: 56.0\n",
      "Level 3: 28.0\n",
      "Level 4: 14.0\n",
      "Level 5: 7.0\n",
      "Max-level: 5\n"
     ]
    }
   ],
   "source": [
    "shape = 224\n",
    "\n",
    "\n",
    "def compute_max_depth(shape, max_depth=10, print_out=True):\n",
    "    shapes = []\n",
    "    shapes.append(shape)\n",
    "    for level in range(1, max_depth):\n",
    "        if shape % 2 ** level == 0 and shape / 2 ** level > 1:\n",
    "            shapes.append(shape / 2 ** level)\n",
    "            if print_out:\n",
    "                print(f'Level {level}: {shape / 2 ** level}')\n",
    "        else:\n",
    "            if print_out:\n",
    "                print(f'Max-level: {level - 1}')\n",
    "            break\n",
    "\n",
    "    return shapes\n",
    "\n",
    "\n",
    "out = compute_max_depth(shape, print_out=True, max_depth=10)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "551006f593f60aa0d3ebb8d2221b5344228da908a7ee96854d832a49ffa0de6f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
