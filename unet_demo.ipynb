{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: torch.Size([16, 3, 240, 240])\n",
      "Out: torch.Size([16, 3, 240, 240])\n",
      "Features: torch.Size([16, 128, 60, 60])\n"
     ]
    }
   ],
   "source": [
    "from unet import UNet\n",
    "import torch\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
    "# Unet has skip connections removed\n",
    "model = UNet(in_channels=3,\n",
    "             out_channels=3,\n",
    "             n_blocks=3,\n",
    "             start_filters=32,\n",
    "             activation='relu',\n",
    "             normalization='batch',\n",
    "             conv_mode='same',\n",
    "             dim=2)\n",
    "model = model.to(device)\n",
    "\n",
    "input = torch.randn(size=(16, 3, 240, 240), dtype=torch.float32)\n",
    "input = input.to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "  out = model(input, encode_only=False)\n",
    "  features = model(input, encode_only=True)\n",
    "\n",
    "print(f'Input: {input.shape}')\n",
    "print(f'Out: {out.shape}')\n",
    "print(f'Features: {features.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 32, 240, 240]             896\n",
      "              ReLU-2         [-1, 32, 240, 240]               0\n",
      "       BatchNorm2d-3         [-1, 32, 240, 240]              64\n",
      "            Conv2d-4         [-1, 32, 240, 240]           9,248\n",
      "              ReLU-5         [-1, 32, 240, 240]               0\n",
      "       BatchNorm2d-6         [-1, 32, 240, 240]              64\n",
      "         MaxPool2d-7         [-1, 32, 120, 120]               0\n",
      "         DownBlock-8  [[-1, 32, 120, 120], [-1, 32, 240, 240]]               0\n",
      "            Conv2d-9         [-1, 64, 120, 120]          18,496\n",
      "             ReLU-10         [-1, 64, 120, 120]               0\n",
      "      BatchNorm2d-11         [-1, 64, 120, 120]             128\n",
      "           Conv2d-12         [-1, 64, 120, 120]          36,928\n",
      "             ReLU-13         [-1, 64, 120, 120]               0\n",
      "      BatchNorm2d-14         [-1, 64, 120, 120]             128\n",
      "        MaxPool2d-15           [-1, 64, 60, 60]               0\n",
      "        DownBlock-16  [[-1, 64, 60, 60], [-1, 64, 120, 120]]               0\n",
      "           Conv2d-17          [-1, 128, 60, 60]          73,856\n",
      "             ReLU-18          [-1, 128, 60, 60]               0\n",
      "      BatchNorm2d-19          [-1, 128, 60, 60]             256\n",
      "           Conv2d-20          [-1, 128, 60, 60]         147,584\n",
      "             ReLU-21          [-1, 128, 60, 60]               0\n",
      "      BatchNorm2d-22          [-1, 128, 60, 60]             256\n",
      "        MaxPool2d-23          [-1, 128, 30, 30]               0\n",
      "        DownBlock-24  [[-1, 128, 30, 30], [-1, 128, 60, 60]]               0\n",
      "           Conv2d-25          [-1, 256, 30, 30]         295,168\n",
      "             ReLU-26          [-1, 256, 30, 30]               0\n",
      "      BatchNorm2d-27          [-1, 256, 30, 30]             512\n",
      "           Conv2d-28          [-1, 256, 30, 30]         590,080\n",
      "             ReLU-29          [-1, 256, 30, 30]               0\n",
      "      BatchNorm2d-30          [-1, 256, 30, 30]             512\n",
      "        MaxPool2d-31          [-1, 256, 15, 15]               0\n",
      "        DownBlock-32  [[-1, 256, 15, 15], [-1, 256, 30, 30]]               0\n",
      "           Conv2d-33          [-1, 512, 15, 15]       1,180,160\n",
      "             ReLU-34          [-1, 512, 15, 15]               0\n",
      "      BatchNorm2d-35          [-1, 512, 15, 15]           1,024\n",
      "           Conv2d-36          [-1, 512, 15, 15]       2,359,808\n",
      "             ReLU-37          [-1, 512, 15, 15]               0\n",
      "      BatchNorm2d-38          [-1, 512, 15, 15]           1,024\n",
      "        DownBlock-39  [[-1, 512, 15, 15], [-1, 512, 15, 15]]               0\n",
      "  ConvTranspose2d-40          [-1, 256, 30, 30]         524,544\n",
      "             ReLU-41          [-1, 256, 30, 30]               0\n",
      "      BatchNorm2d-42          [-1, 256, 30, 30]             512\n",
      "           Conv2d-43          [-1, 256, 30, 30]         590,080\n",
      "             ReLU-44          [-1, 256, 30, 30]               0\n",
      "      BatchNorm2d-45          [-1, 256, 30, 30]             512\n",
      "           Conv2d-46          [-1, 256, 30, 30]         590,080\n",
      "             ReLU-47          [-1, 256, 30, 30]               0\n",
      "      BatchNorm2d-48          [-1, 256, 30, 30]             512\n",
      "          UpBlock-49          [-1, 256, 30, 30]               0\n",
      "  ConvTranspose2d-50          [-1, 128, 60, 60]         131,200\n",
      "             ReLU-51          [-1, 128, 60, 60]               0\n",
      "      BatchNorm2d-52          [-1, 128, 60, 60]             256\n",
      "           Conv2d-53          [-1, 128, 60, 60]         147,584\n",
      "             ReLU-54          [-1, 128, 60, 60]               0\n",
      "      BatchNorm2d-55          [-1, 128, 60, 60]             256\n",
      "           Conv2d-56          [-1, 128, 60, 60]         147,584\n",
      "             ReLU-57          [-1, 128, 60, 60]               0\n",
      "      BatchNorm2d-58          [-1, 128, 60, 60]             256\n",
      "          UpBlock-59          [-1, 128, 60, 60]               0\n",
      "  ConvTranspose2d-60         [-1, 64, 120, 120]          32,832\n",
      "             ReLU-61         [-1, 64, 120, 120]               0\n",
      "      BatchNorm2d-62         [-1, 64, 120, 120]             128\n",
      "           Conv2d-63         [-1, 64, 120, 120]          36,928\n",
      "             ReLU-64         [-1, 64, 120, 120]               0\n",
      "      BatchNorm2d-65         [-1, 64, 120, 120]             128\n",
      "           Conv2d-66         [-1, 64, 120, 120]          36,928\n",
      "             ReLU-67         [-1, 64, 120, 120]               0\n",
      "      BatchNorm2d-68         [-1, 64, 120, 120]             128\n",
      "          UpBlock-69         [-1, 64, 120, 120]               0\n",
      "  ConvTranspose2d-70         [-1, 32, 240, 240]           8,224\n",
      "             ReLU-71         [-1, 32, 240, 240]               0\n",
      "      BatchNorm2d-72         [-1, 32, 240, 240]              64\n",
      "           Conv2d-73         [-1, 32, 240, 240]           9,248\n",
      "             ReLU-74         [-1, 32, 240, 240]               0\n",
      "      BatchNorm2d-75         [-1, 32, 240, 240]              64\n",
      "           Conv2d-76         [-1, 32, 240, 240]           9,248\n",
      "             ReLU-77         [-1, 32, 240, 240]               0\n",
      "      BatchNorm2d-78         [-1, 32, 240, 240]              64\n",
      "          UpBlock-79         [-1, 32, 240, 240]               0\n",
      "           Conv2d-80          [-1, 3, 240, 240]              99\n",
      "================================================================\n",
      "Total params: 6,983,651\n",
      "Trainable params: 6,983,651\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.66\n",
      "Forward/backward pass size (MB): 8707064.94\n",
      "Params size (MB): 26.64\n",
      "Estimated Total Size (MB): 8707092.24\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # PyTorch v0.4.0\n",
    "model = model.to(device)\n",
    "\n",
    "summary = summary(model, (3, 240, 240))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "551006f593f60aa0d3ebb8d2221b5344228da908a7ee96854d832a49ffa0de6f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
