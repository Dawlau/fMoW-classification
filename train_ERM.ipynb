{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wilds import get_dataset\n",
    "from wilds.common.data_loaders import get_train_loader\n",
    "from wilds.common.grouper import CombinatorialGrouper\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from helpers import *\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "from Classifier import Classifier\n",
    "from SubsampledDataset import SubsampledDataset\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# device = \"cpu\"\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Load train and validation data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_dataset(dataset=\"fmow\", download=False)\n",
    "grouper = CombinatorialGrouper(dataset, [\"region\"])\n",
    "\n",
    "ood_train_data = dataset.get_subset(\n",
    "    \"train\",\n",
    "    transform=transforms.Compose(\n",
    "        [transforms.ToTensor()]\n",
    "    ),\n",
    ")\n",
    "\n",
    "ood_val_data = dataset.get_subset(\n",
    "    \"val\",\n",
    "    transform=transforms.Compose(\n",
    "        [transforms.ToTensor()]\n",
    "    ),\n",
    ")\n",
    "\n",
    "id_val_data = dataset.get_subset(\n",
    "    \"id_val\",\n",
    "    transform=transforms.Compose(\n",
    "        [transforms.ToTensor()]\n",
    "    ),\n",
    ")\n",
    "\n",
    "ood_train_dataset = SubsampledDataset(ood_train_data, grouper)\n",
    "ood_val_dataset = SubsampledDataset(ood_val_data, grouper)\n",
    "id_val_dataset = SubsampledDataset(id_val_data, grouper)\n",
    "\n",
    "ood_train_loader = get_train_loader(\"standard\", ood_train_dataset, batch_size=BATCH_SIZE)\n",
    "ood_val_loader   = get_train_loader(\"standard\", ood_val_dataset, batch_size=BATCH_SIZE)\n",
    "id_val_loader    = get_train_loader(\"standard\", id_val_dataset, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Train and accumulate evaluation per epoch</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 2\n",
    "LEARNING_RATE = 0.0001\n",
    "WEIGHT_DECAY = 0.96\n",
    "MODEL_PATH = \"models\"\n",
    "NUM_CLASSES = 20\n",
    "\n",
    "\n",
    "model = Classifier(NUM_CLASSES)\n",
    "model.to(device)\n",
    "\n",
    "model_name = f\"ERM_{NUM_EPOCHS}_Adam_{LEARNING_RATE}_{WEIGHT_DECAY}_CrossEntropy.pth\"\n",
    "save_name = os.path.join(MODEL_PATH, model_name)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "train_evolution = []\n",
    "val_evolution = []\n",
    "id_val_evolution = []\n",
    "best_loss = sys.float_info.max\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "\tprint(f\"EPOCH {epoch + 1}:\")\n",
    "\t# train\n",
    "\ty_true, y_pred, metadata, loss = train_step(model, ood_train_loader, loss_fn, optimizer, device)\n",
    "\ttrain_evolution.append(build_metrics_dict(dataset, y_true, y_pred, metadata, loss))\n",
    "\n",
    "\t# validation\n",
    "\ty_true, y_pred, metadata, loss = val_step(model, ood_val_loader, loss_fn, device)\n",
    "\tval_evolution.append(build_metrics_dict(dataset, y_true, y_pred, metadata, loss))\n",
    "\tprint(f\"OOD Loss: {loss}\")\n",
    "\n",
    "\t# # save by best ood loss\n",
    "\t# if loss < best_loss:\n",
    "\t# \tbest_loss = loss\n",
    "\t# \ttorch.save(model, save_name)\n",
    "\n",
    "\t# in distribution validation\n",
    "\ty_true, y_pred, metadata, loss = val_step(model, id_val_loader, loss_fn, device)\n",
    "\tid_val_evolution.append(build_metrics_dict(dataset, y_true, y_pred, metadata, loss))\n",
    "\tprint(f\"ID Loss: {loss}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Plot loss and accuracy per region</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = list(train_evolution[0].keys())\n",
    "\n",
    "for metric in metrics:\n",
    "\tplot_graph(metric, train_evolution, val_evolution, id_val_evolution, NUM_EPOCHS)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "ff1607acb86e9c06c30b27f75fb57134555e2c7e307c575a25fc991546615253"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('fmow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
