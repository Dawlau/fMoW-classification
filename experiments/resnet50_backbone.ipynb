{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wilds import get_dataset\n",
    "from wilds.common.data_loaders import get_train_loader\n",
    "from wilds.common.grouper import CombinatorialGrouper\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(\"..\", \"helpers\"))\n",
    "\n",
    "from erm_helpers import *\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Load train and validation data</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = get_dataset(dataset=\"fmow\", download=False)\n",
    "\n",
    "ood_train_data = dataset.get_subset(\n",
    "    \"train\",\n",
    "    transform=transforms.Compose(\n",
    "        [transforms.ToTensor()]\n",
    "    ),\n",
    ")\n",
    "\n",
    "ood_val_data = dataset.get_subset(\n",
    "    \"val\",\n",
    "    transform=transforms.Compose(\n",
    "        [transforms.ToTensor()]\n",
    "    ),\n",
    ")\n",
    "\n",
    "id_val_data = dataset.get_subset(\n",
    "    \"id_val\",\n",
    "    transform=transforms.Compose(\n",
    "        [transforms.ToTensor()]\n",
    "    ),\n",
    ")\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "ood_train_loader = get_train_loader(\"standard\", ood_train_data, batch_size=BATCH_SIZE)\n",
    "ood_val_loader   = get_train_loader(\"standard\", ood_val_data, batch_size=BATCH_SIZE)\n",
    "id_val_loader    = get_train_loader(\"standard\", id_val_data, batch_size=BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Define the resnet50 backbone model</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet50_Backbone(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(ResNet50_Backbone, self).__init__()\n",
    "\n",
    "\t\tself.resnet50 = torchvision.models.resnet50(pretrained=True)\n",
    "\t\tself.resnet50 = nn.Sequential(*(list(self.resnet50.children())[:-1])) # remove fc layer from resnet50\n",
    "\t\tfor parameter in self.resnet50.parameters():\n",
    "\t\t\tparameter.requires_grad = False\n",
    "\n",
    "\t\tself.model = nn.Sequential(\n",
    "\t\t\tnn.Conv1d(1, 1, kernel_size=3, padding=1),\n",
    "\t\t\tnn.Dropout(0.4),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.MaxPool1d(8),\n",
    "\t\t\tnn.BatchNorm1d(1),\n",
    "\t\t\tnn.Conv1d(1, 1, kernel_size=3, padding=1),\n",
    "\t\t\tnn.Dropout(0.3),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.MaxPool1d(4),\n",
    "\t\t\tnn.BatchNorm1d(1),\n",
    "\t\t\tnn.Flatten(),\n",
    "\t\t\tnn.Linear(64, 62),\n",
    "\t\t\tnn.Dropout(0.5),\n",
    "\t\t\tnn.Sigmoid()\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tresnet_features = self.resnet50(x)\n",
    "\t\tresnet_features = torch.squeeze(resnet_features)\n",
    "\t\tresnet_features = torch.unsqueeze(resnet_features, dim=1)\n",
    "\t\toutput = self.model(resnet_features)\n",
    "\t\treturn output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Train and accumulate evaluation per epoch</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 30\n",
    "LEARNING_RATE = 0.0001\n",
    "WEIGHT_DECAY = 0.96\n",
    "MODEL_PATH = os.path.join(\"models\", \"resnet50_backbone\")\n",
    "\n",
    "model = ResNet50_Backbone()\n",
    "model.to(device)\n",
    "\n",
    "model_name = f\"resnet50_backbone_conv_overhead_sigmoid_{NUM_EPOCHS}_Adam_{LEARNING_RATE}_CrossEntropy.pt\"\n",
    "save_name = os.path.join(MODEL_PATH, model_name)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "train_evolution = []\n",
    "val_evolution = []\n",
    "id_val_evolution = []\n",
    "best_loss = sys.float_info.max\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "\tprint(f\"EPOCH {epoch + 1}:\")\n",
    "\t# train\n",
    "\ty_true, y_pred, metadata, loss = train_step(model, ood_train_loader, loss_fn, optimizer, device)\n",
    "\ttrain_evolution.append(build_metrics_dict(dataset, y_true, y_pred, metadata, loss))\n",
    "\n",
    "\t# validation\n",
    "\ty_true, y_pred, metadata, loss = val_step(model, ood_val_loader, loss_fn, device)\n",
    "\tval_evolution.append(build_metrics_dict(dataset, y_true, y_pred, metadata, loss))\n",
    "\tprint(f\"OOD Loss: {loss}\")\n",
    "\n",
    "\t# save by best ood loss\n",
    "\tif loss < best_loss:\n",
    "\t\tbest_loss = loss\n",
    "\t\ttorch.save(model, save_name)\n",
    "\n",
    "\t# in distribution validation\n",
    "\ty_true, y_pred, metadata, loss = val_step(model, id_val_loader, loss_fn, device)\n",
    "\tid_val_evolution.append(build_metrics_dict(dataset, y_true, y_pred, metadata, loss))\n",
    "\tprint(f\"ID Loss: {loss}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">F1 score on validation datasets</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true, y_pred, metadata, loss = val_step(model, id_val_loader, loss_fn, device)\n",
    "f1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "print(\"In distribution F1 score: \", f1)\n",
    "\n",
    "y_true, y_pred, metadata, loss = val_step(model, ood_val_loader, loss_fn, device)\n",
    "f1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "print(\"Out of distribution F1 score: \", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"6\">Plot loss and accuracy per region</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = list(train_evolution[0].keys())\n",
    "\n",
    "for metric in metrics:\n",
    "\tplot_graph(metric, train_evolution, val_evolution, id_val_evolution, NUM_EPOCHS)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9088b351a5a617a459921d0866f072b197f5cdce1d55c463114270062702b24c"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 ('wilds')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
